{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a4a32f6",
   "metadata": {},
   "source": [
    "# Extract metadata from PDF files using fine-tuned GPT3 language model\n",
    "\n",
    "This notebook will demonstrate how we can extract Dublin Core style metadata about PDF documents, in this case doctoral theses from four Finnish universities (√Öbo Akademi, University of Turku, University of Vaasa and Lappeenranta University of Technology), using only the raw text from the first few pages of the PDF.\n",
    "\n",
    "The set of 192 documents will be split into two subsets (train: 149, test: 43). We will extract the text from around 5 pages of text, aiming for 500 to 700 words. The corresponding metadata, which has been exported from DSpace repositories of the universities, is represented in a simple textual \"key: value\" format, which should be easy enough for a language model to handle. The train set is used to create a data set which will then be used to fine-tune a GPT-3 Curie model. Subsequently the model can be used to generate similar metadata for unseen documents from the test set.\n",
    "\n",
    "For this experiment, an OpenAI API access key is required. It can be generated after registering an user account (the same account can be used for e.g. ChatGPT). The API key has to be stored in an environment variable `OPENAI_API_KEY` before starting this notebook. The finetuning will cost around \\\\$2.50 USD and generating new metadata with the API also has a small cost, but currently every account gets a free \\\\$18 credit from OpenAI which is plenty for this experiment even with a few iterations.\n",
    "\n",
    "This notebook depends on a few Python libraries, which are listed in `requirements.txt`. See the README for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c63d21",
   "metadata": {},
   "source": [
    "## Test the connection and API key\n",
    "\n",
    "Make sure it's possible to use the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24ff64b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-6bmAgQqLkF71IJL0qqBELSTuq2P2h at 0x7fd50f537060> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"length\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \"\\n\\nThis is a test.\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1674461326,\n",
       "  \"id\": \"cmpl-6bmAgQqLkF71IJL0qqBELSTuq2P2h\",\n",
       "  \"model\": \"text-curie-001\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 7,\n",
       "    \"prompt_tokens\": 5,\n",
       "    \"total_tokens\": 12\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# read the OpenAI API key from an environment variable\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# test the API connection by making a simple request\n",
    "response = openai.Completion.create(model=\"text-curie-001\", prompt=\"Say this is a test\", temperature=0, max_tokens=7)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73db4c29",
   "metadata": {},
   "source": [
    "## Prepare the data set\n",
    "\n",
    "Extract metadata and PDF text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6f64d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some settings for the metadata extraction\n",
    "\n",
    "import glob\n",
    "\n",
    "MAXPAGES = 5  # how many pages of text to extract (maximum)\n",
    "MARGIN = 2  # how many more pages to look at, in case we can't find text from the first ones\n",
    "TEXT_MIN = 500  # how many words to aim for (minimum)\n",
    "TEXT_LIMIT = 700  # upper limit on # of words\n",
    "\n",
    "# files containing metadata about doctoral theses documents, exported from DSpace repositories\n",
    "METADATAFILES = glob.glob(\"dspace/*-doctheses.xml\")\n",
    "\n",
    "# metadata fields we are interested in (corresponding to fields used in DSpace)\n",
    "# syntax: \"fieldname\" or \"fieldname/qualifier\"\n",
    "METADATAFIELDS = \"\"\"\n",
    "title\n",
    "title/alternative\n",
    "contributor/faculty\n",
    "contributor/author\n",
    "contributor/organization\n",
    "contributor/opponent\n",
    "contributor/supervisor\n",
    "contributor/reviewer\n",
    "publisher\n",
    "date/issued\n",
    "relation/issn\n",
    "relation/isbn\n",
    "relation/ispartofseries\n",
    "relation/numberinseries\n",
    "\"\"\".strip().split()\n",
    "\n",
    "# identifiers of documents that will form the test set\n",
    "# these have been selected to correspond with a demo application for the same purpose\n",
    "TEST_SET_IDS = \"\"\"\n",
    "handle_10024_181378\n",
    "handle_10024_181284\n",
    "handle_10024_181280\n",
    "handle_10024_181229\n",
    "handle_10024_181227\n",
    "handle_10024_181210\n",
    "handle_10024_181206\n",
    "handle_10024_181139\n",
    "handle_10024_181073\n",
    "handle_10024_181025\n",
    "handle_10024_181001\n",
    "handle_10024_163335\n",
    "handle_10024_163304\n",
    "handle_10024_163298\n",
    "handle_10024_163277\n",
    "handle_10024_163276\n",
    "handle_10024_163263\n",
    "handle_10024_163258\n",
    "handle_10024_163257\n",
    "handle_10024_163057\n",
    "handle_10024_163056\n",
    "handle_10024_162878\n",
    "handle_10024_11364\n",
    "handle_10024_11363\n",
    "handle_10024_11348\n",
    "handle_10024_11207\n",
    "handle_10024_10928\n",
    "handle_10024_10620\n",
    "handle_10024_10614\n",
    "handle_10024_10443\n",
    "handle_10024_10432\n",
    "handle_10024_10254\n",
    "handle_10024_152922\n",
    "handle_10024_152903\n",
    "handle_10024_152904\n",
    "handle_10024_152860\n",
    "handle_10024_152862\n",
    "handle_10024_152853\n",
    "handle_10024_152854\n",
    "handle_10024_152855\n",
    "handle_10024_152852\n",
    "handle_10024_152846\n",
    "handle_10024_152836\n",
    "\"\"\".strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ed0cf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_13521.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_13449.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_13420.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_13378.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_13377.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_13257.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_13205.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_13177.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_13169.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_13167.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_13159.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_13153.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_13152.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_13119.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_13116.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_12958.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_12619.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_12481.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_12480.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_12300.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_12272.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_12259.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_11784.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_11593.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_11584.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_11561.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_11491.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_11407.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_11363.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_11348.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_11207.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_10614.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_10443.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_10432.pdf: text would become too long\n",
      "skipping page 5 of docs/www.utupub.fi_handle_10024_153232.pdf: text would become too long\n",
      "skipping page 5 of docs/www.utupub.fi_handle_10024_153200.pdf: text would become too long\n",
      "skipping page 5 of docs/www.doria.fi_handle_10024_182724.pdf: text would become too long\n",
      "skipping page 6 of docs/www.doria.fi_handle_10024_182724.pdf: text would become too long\n",
      "skipping page 5 of docs/www.doria.fi_handle_10024_182159.pdf: text would become too long\n",
      "skipping page 6 of docs/www.doria.fi_handle_10024_182159.pdf: text would become too long\n",
      "skipping page 5 of docs/www.doria.fi_handle_10024_181975.pdf: text would become too long\n",
      "skipping page 6 of docs/www.doria.fi_handle_10024_181975.pdf: text would become too long\n",
      "skipping page 5 of docs/www.doria.fi_handle_10024_181902.pdf: text would become too long\n",
      "skipping page 6 of docs/www.doria.fi_handle_10024_181902.pdf: text would become too long\n",
      "skipping page 7 of docs/www.doria.fi_handle_10024_181902.pdf: text would become too long\n",
      "skipping page 5 of docs/www.doria.fi_handle_10024_181847.pdf: text would become too long\n",
      "skipping page 5 of docs/www.doria.fi_handle_10024_181727.pdf: text would become too long\n",
      "no file element found (id: https://www.doria.fi/handle/10024/181511), skipping\n",
      "skipping page 5 of docs/www.doria.fi_handle_10024_181280.pdf: text would become too long\n",
      "skipping page 6 of docs/www.doria.fi_handle_10024_181280.pdf: text would become too long\n",
      "skipping page 7 of docs/www.doria.fi_handle_10024_181280.pdf: text would become too long\n",
      "skipping page 5 of docs/www.doria.fi_handle_10024_181227.pdf: text would become too long\n",
      "skipping page 5 of docs/www.doria.fi_handle_10024_181073.pdf: text would become too long\n",
      "no file element found (id: https://lutpub.lut.fi/handle/10024/163756), skipping\n",
      "train set size: 149\n",
      "test set size: 43\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "\n",
    "from lxml import etree\n",
    "import requests\n",
    "import os.path\n",
    "from pypdf import PdfReader\n",
    "import glob\n",
    "\n",
    "# train set: document identifiers, text (x) and metadata (y)\n",
    "train_ids = []\n",
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "# test set: document identifiers, text (x) and metadata (y)\n",
    "test_ids = []\n",
    "test_x = []\n",
    "test_y = []\n",
    "\n",
    "def extract_metadata(doc_item):\n",
    "    \"\"\"extract the metadata as a list of (key, value) tuples from an etree element representing a document\"\"\"\n",
    "    metadata = []\n",
    "    for fldname in METADATAFIELDS:\n",
    "        if '/' in fldname:\n",
    "            fld, qualifier = fldname.split('/')\n",
    "            for val in doc_item.findall(f\"metadata[@element='{fld}'][@qualifier='{qualifier}']\"):\n",
    "                if fld == 'date':\n",
    "                    metadata.append((fldname, val.text[:4]))  # only the year                \n",
    "                else:\n",
    "                    metadata.append((fldname, val.text))\n",
    "        else:\n",
    "            for val in doc_item.findall(f\"metadata[@element='{fldname}'][@qualifier='']\"):\n",
    "                metadata.append((fldname, val.text))\n",
    "    return metadata\n",
    "\n",
    "def id_to_fn(identifier):\n",
    "    \"\"\"convert a URI identifier to a simpler string we can use as a filename for the PDF\"\"\"\n",
    "    return 'docs/' + identifier.replace('https://', '').replace('/','_') + \".pdf\"\n",
    "\n",
    "def download(file_url, identifier):\n",
    "    \"\"\"download a PDF file, with the given identifier, from the given URL (unless this was done already)\n",
    "    and return a path to the PDF file\"\"\"\n",
    "    path = id_to_fn(identifier)\n",
    "    if os.path.exists(path) and os.path.getsize(path) > 0:\n",
    "        return path\n",
    "\n",
    "    response = requests.get(file_url)\n",
    "    with open(path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "        print(f\"wrote {file_url} as {path}\")\n",
    "    return path\n",
    "\n",
    "def extract_text(fn):\n",
    "    \"\"\"extract and return the first few pages of text from the given PDF file\"\"\"\n",
    "    reader = PdfReader(fn)\n",
    "    texts = []\n",
    "    extracted_pages = 0\n",
    "    extracted_length = 0\n",
    "    for idx, page in enumerate(reader.pages[:MAXPAGES + MARGIN]):\n",
    "        text = page.extract_text()\n",
    "        text_length = len(text.strip().split())        \n",
    "        if extracted_length + text_length < TEXT_LIMIT:\n",
    "            texts.append(text)\n",
    "            extracted_length += text_length\n",
    "            extracted_pages += 1\n",
    "        else:\n",
    "            print(f\"skipping page {idx+1} of {fn}: text would become too long\")\n",
    "        if extracted_pages >= MAXPAGES or extracted_length >= TEXT_MIN:\n",
    "            break\n",
    "    return '\\n'.join(texts)\n",
    "\n",
    "def is_test_doc(identifier):\n",
    "    \"\"\"return True iff the given identifier belongs to the test set\"\"\"\n",
    "    shortid = 'handle' + identifier.split('handle')[1].replace('/', '_')\n",
    "    return shortid in TEST_SET_IDS    \n",
    "\n",
    "# Read all the metadata files, extract the DSpace metadata, download the PDFs and extract text from them\n",
    "# into the train_* and test_* lists\n",
    "for fn in METADATAFILES:\n",
    "    tree = etree.parse(fn)\n",
    "    for item in tree.findall('item'):\n",
    "        try:\n",
    "            identifier = item.find(\"metadata[@element='identifier'][@qualifier='uri']\").text\n",
    "        except AttributeError:\n",
    "            print(\"no identifier found, skipping\")\n",
    "            continue\n",
    "        try:\n",
    "            file_url = item.find('file').text\n",
    "        except AttributeError:\n",
    "            print(f\"no file element found (id: {identifier}), skipping\")\n",
    "            continue\n",
    "            print(f\"skipping test document {identifier}\")\n",
    "            continue\n",
    "        path = download(file_url, identifier)\n",
    "        text = extract_text(path)\n",
    "        metadata = extract_metadata(item)\n",
    "        if is_test_doc(identifier):\n",
    "            test_ids.append(identifier)\n",
    "            test_x.append(text)\n",
    "            test_y.append(metadata)\n",
    "        else:\n",
    "            train_ids.append(identifier)\n",
    "            train_x.append(text)\n",
    "            train_y.append(metadata)\n",
    "\n",
    "print(f\"train set size: {len(train_ids)}\")\n",
    "print(f\"test set size: {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1daf3fc",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "\n",
    "Prepare a fine-tuning dataset and use it to fine-tune a GPT3 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14f9952f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote fine-tuning data set into file fine-tune.jsonl\n"
     ]
    }
   ],
   "source": [
    "# prepare fine-tuning dataset\n",
    "import json\n",
    "\n",
    "PROMPT_SUFFIX = '\\n\\n###\\n\\n'\n",
    "COMPLETION_STOP = '\\n###'\n",
    "TRAINFILE = 'fine-tune.jsonl'\n",
    "\n",
    "def metadata_to_text(metadata):\n",
    "    \"\"\"convert the metadata tuple to text with key: value pairs\"\"\"\n",
    "    return \"\\n\".join([f\"{fld}: {val}\" for fld, val in metadata])\n",
    "\n",
    "def create_sample(text, metadata):\n",
    "    \"\"\"create a fine-tuning sample from text and metadata about a single document\"\"\"\n",
    "    return {'prompt': text + PROMPT_SUFFIX,\n",
    "            'completion': metadata_to_text(metadata) + COMPLETION_STOP}\n",
    "\n",
    "with open(TRAINFILE, 'w') as outf:\n",
    "    for text, metadata in zip(train_x, train_y):\n",
    "        sample = create_sample(text, metadata)\n",
    "        print(json.dumps(sample), file=outf)\n",
    "\n",
    "print(f\"wrote fine-tuning data set into file {TRAINFILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "461a296d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing...\n",
      "\n",
      "- Your file contains 149 prompt-completion pairs\n",
      "- All prompts end with suffix `\\n\\n###\\n\\n`\n",
      "- All completions start with prefix `title: `. Most of the time you should only add the output data into the completion, without any prefix\n",
      "- All completions end with suffix `\\n###`\n",
      "- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
      "\n",
      "Based on the analysis we will perform the following actions:\n",
      "- [Recommended] Remove prefix `title: ` from all completions [Y/n]: ^C\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Optional:\n",
    "# Check that the fine-tuning data set is OK using the prepare_data tool.\n",
    "# It will complain that all completions start with the same \"title:\" prefix, this can be ignored.\n",
    "# NOTE: The command has to be interrupted by pressing the stop button in Jupyter.\n",
    "!openai tools fine_tunes.prepare_data -f fine-tune.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e03d7a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 643k/643k [00:00<00:00, 554Mit/s]\n",
      "Uploaded file from fine-tune.jsonl: file-qggpGW5qlkmO5Moe0DzGP4Hw\n",
      "Created fine-tune: ft-iwX4ajAH4TaptYpJdXFSSOu2\n",
      "Streaming events until fine-tuning is complete...\n",
      "\n",
      "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
      "[2023-01-23 10:18:08] Created fine-tune: ft-iwX4ajAH4TaptYpJdXFSSOu2\n",
      "\n",
      "Stream interrupted (client disconnected).\n",
      "To resume the stream, run:\n",
      "\n",
      "  openai api fine_tunes.follow -i ft-iwX4ajAH4TaptYpJdXFSSOu2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform the actual finetuning via the API. This can take a while, there can be a long queue.\n",
    "!openai api fine_tunes.create -t fine-tune.jsonl -m curie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2327e76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-01-23 10:18:08] Created fine-tune: ft-iwX4ajAH4TaptYpJdXFSSOu2\n",
      "[2023-01-23 10:22:29] Fine-tune costs $2.40\n",
      "[2023-01-23 10:22:29] Fine-tune enqueued. Queue number: 1\n",
      "[2023-01-23 10:22:30] Fine-tune is in the queue. Queue number: 0\n",
      "[2023-01-23 10:22:38] Fine-tune started\n",
      "[2023-01-23 10:24:36] Completed epoch 1/4\n",
      "[2023-01-23 10:25:56] Completed epoch 2/4\n",
      "[2023-01-23 10:27:16] Completed epoch 3/4\n",
      "[2023-01-23 10:28:38] Completed epoch 4/4\n",
      "[2023-01-23 10:29:01] Uploaded model: curie:ft-personal-2023-01-23-08-29-00\n",
      "[2023-01-23 10:29:02] Uploaded result file: file-ZDZOAruJOzm3JPYQMfjIQmIu\n",
      "[2023-01-23 10:29:02] Fine-tune succeeded\n",
      "\n",
      "Job complete! Status: succeeded üéâ\n",
      "Try out your fine-tuned model:\n",
      "\n",
      "openai api completions.create -m curie:ft-personal-2023-01-23-08-29-00 -p <YOUR_PROMPT>\n"
     ]
    }
   ],
   "source": [
    "# reattach in case the stream gets interrupted\n",
    "#!openai api fine_tunes.follow -i <insert finetuning job id here>\n",
    "!openai api fine_tunes.follow -i ft-iwX4ajAH4TaptYpJdXFSSOu2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9376e9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the model name from above output and store it in a variable\n",
    "model_name = \"curie:ft-personal-2023-01-23-08-29-00\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4285709c",
   "metadata": {},
   "source": [
    "## Test the fine-tuned model\n",
    "\n",
    "Give the model some documents from the test set that it has never seen before and see what kind of metadata it can extract. Compare that to the manually created metadata of the same documents, extracted from DSpace systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8cba90a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://osuva.uwasa.fi/handle/10024/11207\n",
      "---\n",
      "DSpace metadata:\n",
      "title: How to apply technology, knowledge and operations decision models for strategically sustainable resource allocation?\n",
      "title/alternative: Kuinka soveltaa teknologiaan, tietoon ja tuotantoon liittyv√§n p√§√§t√∂ksenteon malleja strategisesti kest√§v√§√§n resurssien allokointiin?\n",
      "contributor/faculty: fi=Tekniikan ja innovaatiojohtamisen yksikk√∂|en=School of Technology and Innovations|\n",
      "contributor/author: Tilabi, Sara\n",
      "contributor/organization: fi=Vaasan yliopisto|en=University of Vaasa|\n",
      "publisher: Vaasan yliopisto\n",
      "date/issued: 2020\n",
      "relation/issn: 2323-9123\n",
      "relation/issn: 0355-2667\n",
      "relation/isbn: 978-952-476-915-0\n",
      "relation/ispartofseries: Acta Wasaensia\n",
      "relation/numberinseries: 445\n",
      "---\n",
      "Generated metadata:\n",
      "title: How to apply technology, knowledge and operations decision models for strategically sustainable resource allocation?\n",
      "title/alternative: Kuinka soveltaa teknologiaan, tietoon ja tuotantoon liittyv√§√§ p√§√§t√∂ksen teon malleja strategisesti kest√§v√§√§n resurssien allokointiin?\n",
      "contributor/faculty: fi=Tekniikan ja innovaatiojohtamisen yksikk√∂|en=School of Technology and Innovations|\n",
      "contributor/author: Tilabi, Sara\n",
      "contributor/organization: fi=Vaasan yliopisto|en=University of Vaasa|\n",
      "publisher: Vaasan yliopisto\n",
      "date/issued: 2020\n",
      "relation/issn: 2323-9123\n",
      "relation/issn: 0355-2667\n",
      "relation/isbn: 978-952-476-915-0\n",
      "relation/ispartofseries: Acta Wasaensia\n",
      "relation/numberinseries: 445\n",
      "\n",
      "https://osuva.uwasa.fi/handle/10024/10432\n",
      "---\n",
      "DSpace metadata:\n",
      "title: Persoonallinen ajattelu p√§√§ttelyss√§ ja p√§√§t√∂ksenteossa\n",
      "title/alternative: Personal Thinking in reasoning and decision making\n",
      "contributor/faculty: fi=Tekniikan ja innovaatiojohtamisen yksikk√∂|en=School of Technology and Innovations|\n",
      "contributor/author: Alho, Tapio\n",
      "contributor/organization: fi=Vaasan yliopisto|en=University of Vaasa|\n",
      "publisher: Vaasan yliopisto\n",
      "date/issued: 2020\n",
      "relation/issn: 2323-9123\n",
      "relation/issn: 0355-2667\n",
      "relation/isbn: 978-952-476-903-7\n",
      "relation/ispartofseries: Acta Wasaensia\n",
      "relation/numberinseries: 440\n",
      "---\n",
      "Generated metadata:\n",
      "title: Persoonallinen ajattelu p√§√§ttelyss√§ ja p√§√§t√∂ksenteossa\n",
      "title/alternative: Personal thinking in judgment and decision-making\n",
      "contributor/faculty: fi=Tekniikan ja innovaatiojohtamisen yksikk√∂|en=School of Technology and Innovations|\n",
      "contributor/author: Alho, Tapio\n",
      "contributor/organization: fi=Vaasan yliopisto|en=University of Vaasa|\n",
      "publisher: Vaasan yliopisto\n",
      "date/issued: 2020\n",
      "relation/issn: 2323-9123\n",
      "relation/issn: 0355-2667\n",
      "relation/isbn: 978-952-476-903-7\n",
      "relation/ispartofseries: Acta Wasaensia\n",
      "relation/numberinseries: 440\n",
      "\n",
      "https://www.utupub.fi/handle/10024/152860\n",
      "---\n",
      "DSpace metadata:\n",
      "title: Essays on income inequality and financial incentives to work\n",
      "contributor/faculty: fi=Turun kauppakorkeakoulu|en=Turku School of Economics|\n",
      "contributor/author: Ollonqvist, Joonas\n",
      "publisher: fi=Turun yliopisto. Turun kauppakorkeakoulu|en=University of Turku, Turku School of Economics|\n",
      "date/issued: 2021\n",
      "relation/issn: 2343-3167\n",
      "relation/ispartofseries: Turun yliopiston julkaisuja - Annales Universitatis Turkuensis, Ser E: Oeconomica\n",
      "relation/numberinseries: 82\n",
      "---\n",
      "Generated metadata:\n",
      "title: Esa: Income inequality and financial incentives to work\n",
      "contributor/faculty: fi=Turun kauppakorkeakoulu|en=Turku School of Economics|\n",
      "contributor/author: Ollonqvist, Joonas\n",
      "publisher: fi=Turun yliopisto. Turun kauppakorkeakoulu|en=University of Turku, Turku School of Economics|\n",
      "date/issued: 2022\n",
      "relation/issn: 2343-3167\n",
      "relation/ispartofseries: Turun yliopiston julkaisuja - Annales Universitatis Turkuensis, Ser E: Oeconomica\n",
      "relation/numberinseries: 82\n",
      "\n",
      "https://www.utupub.fi/handle/10024/152852\n",
      "---\n",
      "DSpace metadata:\n",
      "title: Run-time management of many-core SoCs: A communication-centric approach\n",
      "contributor/faculty: fi=Teknillinen tiedekunta|en=Faculty of Technology|\n",
      "contributor/author: Fattah, Mohammad\n",
      "publisher: fi=Turun yliopisto|en=University of Turku|\n",
      "date/issued: 2021\n",
      "relation/issn: 2736-9684\n",
      "relation/ispartofseries: Turun yliopiston julkaisua - Annales Universitatis Turkuensis, Ser. F: Technica - Informatica\n",
      "relation/numberinseries: 7\n",
      "---\n",
      "Generated metadata:\n",
      "title: Run-time management of many-core systems ‚Äì A communication-centric approach\n",
      "contributor/faculty: fi=Teknillinen tiedekunta|en=Faculty of Technology|\n",
      "contributor/author: Fattah, Mohammad\n",
      "publisher: fi=Turun yliopisto|en=University of Turku|\n",
      "date/issued: 2021\n",
      "relation/issn: 2736-9684\n",
      "relation/ispartofseries: Turun yliopiston julkaisuja - Annales Universitatis Turkuensis, Ser. F: Technica - Informatica\n",
      "relation/numberinseries: 7\n",
      "\n",
      "https://www.doria.fi/handle/10024/181280\n",
      "---\n",
      "DSpace metadata:\n",
      "title: Production and testing of magnesium carbonate hydrates for thermal energy storage (TES) application\n",
      "contributor/faculty: Faculty of Science and Engineering\n",
      "contributor/faculty: Fakulteten f√∂r naturvetenskaper och teknik\n",
      "contributor/faculty: Luonnontieteiden ja tekniikan tiedekunta\n",
      "contributor/author: Erlund, Rickard\n",
      "contributor/opponent: Professor Brian Elmegaard, Technical University of Denmark, Lyngby, Denmark\n",
      "contributor/supervisor: Professor Ron Zevenhoven, √Öbo Akademi University, Turku\n",
      "publisher: √Öbo Akademi University\n",
      "date/issued: 2021\n",
      "---\n",
      "Generated metadata:\n",
      "title: Production and Testing of Magnesium Carbonate Hydrates for Thermal Energy Storage (TES) Application\n",
      "contributor/faculty: Faculty of Science and Engineering\n",
      "contributor/faculty: Fakulteten f√∂r naturvetenskaper och teknik\n",
      "contributor/faculty: Luonnontieteiden ja tekniikan tiedekunta\n",
      "contributor/author: Erlund, Rickard\n",
      "contributor/opponent: Professor, Technical University of Denmark, Lyngby, Denmark\n",
      "contributor/supervisor: Professor, √Öbo Akademi University, Turku\n",
      "contributor/supervisor: Professor, √Öbo Akademi University, Turku\n",
      "publisher: √Öbo Akademi University\n",
      "date/issued: 2021\n",
      "\n",
      "https://www.doria.fi/handle/10024/181139\n",
      "---\n",
      "DSpace metadata:\n",
      "title: Coulometric Transduction Method for Solid-Contact Ion-Selective Electrodes\n",
      "contributor/faculty: Faculty of Science and Engineering\n",
      "contributor/faculty: Fakulteten f√∂r naturvetenskaper och teknik\n",
      "contributor/faculty: Luonnontieteiden ja tekniikan tiedekunta\n",
      "contributor/author: Han, Tingting\n",
      "contributor/opponent: Professor Agata Michalska, University of Warsaw, Warsaw, Poland\n",
      "contributor/supervisor: Professor Johan Bobacka, √Öbo Akademi University, √Öbo\n",
      "contributor/supervisor: Dr. Ulriika Mattinen, √Öbo Akademi University, √Öbo\n",
      "contributor/supervisor: Docent Zekra Mousavi, √Öbo Akademi University, √Öbo\n",
      "publisher: √Öbo Akademi University\n",
      "date/issued: 2021\n",
      "---\n",
      "Generated metadata:\n",
      "title: Coulometric transduction method for solid-contact ion-selective electrodes\n",
      "contributor/faculty: Faculty of Science and Engineering\n",
      "contributor/faculty: Fakulteten f√∂r naturvetenskaper och teknik\n",
      "contributor/faculty: Luonnontieteiden ja tekniikan tiedekunta\n",
      "contributor/author: Han, Tingting\n",
      "contributor/opponent: Professor Agata Michalska, University of Warsaw, Warsaw, Poland\n",
      "contributor/supervisor: Professor Johan Bobacka, √Öbo Akademi University, √Öbo\n",
      "contributor/supervisor: Dr. Ulriika Mattinen, √Öbo Akademi University, √Öbo\n",
      "contributor/supervisor: Docent Zekra Mousavi, √Öbo Akademi University, √Öbo\n",
      "publisher: √Öbo Akademi University\n",
      "date/issued: 2021\n",
      "\n",
      "https://lutpub.lut.fi/handle/10024/163304\n",
      "---\n",
      "DSpace metadata:\n",
      "title: Responsible business practices in internationalized SMEs\n",
      "contributor/faculty: fi=School of Business and Management|en=School of Business and Management|\n",
      "contributor/author: Uzhegova, Maria\n",
      "contributor/organization: Lappeenrannan-Lahden teknillinen yliopisto LUT\n",
      "contributor/organization: Lappeenranta-Lahti University of Technology LUT\n",
      "contributor/opponent: Rialp-Criado, Alex\n",
      "contributor/reviewer: Rialp-Criado, Alex\n",
      "contributor/reviewer: Andersson, Svante\n",
      "date/issued: 2021\n",
      "relation/issn: 1456-4491\n",
      "relation/ispartofseries: Acta Universitatis Lappeenrantaensis\n",
      "---\n",
      "Generated metadata:\n",
      "title: Responsible business practices in internationalized SMEs\n",
      "contributor/faculty: fi=School of Business and Management|en=School of Business and Management|\n",
      "contributor/author: Uzhegova, Maria\n",
      "contributor/organization: Lappeenrannan-Lahden teknillinen yliopisto LUT\n",
      "contributor/organization: Lappeenranta-Lahti University of Technology LUT\n",
      "contributor/opponent: Rialp-Criado, Alex\n",
      "contributor/reviewer: Rialp-Criado, Alex\n",
      "contributor/reviewer: Andersson, Svante\n",
      "publisher: Lappeenranta-Lahti University of Technology LUT\n",
      "date/issued: 2021\n",
      "relation/issn: 1456-4491\n",
      "relation/ispartofseries: Acta Universitatis Lappeenrantaensis\n",
      "\n",
      "https://lutpub.lut.fi/handle/10024/163258\n",
      "---\n",
      "DSpace metadata:\n",
      "title: Life cycle cost-driven design for additive manufacturing : the frontier to sustainable manufacturing in laser-based powder bed fusion\n",
      "contributor/faculty: fi=School of Energy Systems|en=School of Energy Systems|\n",
      "contributor/author: Nyamekye, Patricia\n",
      "contributor/organization: Lappeenrannan-Lahden teknillinen yliopisto LUT\n",
      "contributor/organization: Lappeenranta-Lahti University of Technology LUT\n",
      "contributor/opponent: Hryha, Eduard\n",
      "contributor/reviewer: Hryha, Eduard\n",
      "contributor/reviewer: Mazzi, Anna\n",
      "publisher: Lappeenranta-Lahti University of Technology LUT\n",
      "date/issued: 2021\n",
      "relation/issn: 1456-4491\n",
      "relation/ispartofseries: Acta Universitatis Lappeenrantaensis\n",
      "---\n",
      "Generated metadata:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: Life cycle cost-driven design for additive manufacturing: the frontier to sustainable manufacturing in laser-based powder bed fusion\n",
      "contributor/faculty: fi=School of Energy Systems|en=School of Energy Systems|\n",
      "contributor/author: Nyamekye, Patricia\n",
      "contributor/organization: Lappeenrannan-Lahden teknillinen yliopisto LUT\n",
      "contributor/organization: Lappeenranta-Lahti University of Technology LUT\n",
      "contributor/opponent: Hryha, Eduard\n",
      "contributor/reviewer: Hryha, Eduard\n",
      "contributor/reviewer: Mazzi, Anna\n",
      "publisher: Lappeenranta-Lahti University of Technology LUT\n",
      "date/issued: 2021\n",
      "relation/issn: 1456-4491\n",
      "relation/ispartofseries: Acta Universitatis Lappeenrantaensis\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_completions(text):\n",
    "    response = openai.Completion.create(model=model_name,\n",
    "                                    prompt=text + PROMPT_SUFFIX,\n",
    "                                    temperature=0,  # no fooling around!\n",
    "                                    max_tokens=500, # should be plenty\n",
    "                                    stop=[COMPLETION_STOP])  # stop at ###\n",
    "    return response['choices'][0]['text']\n",
    "\n",
    "# test it with some sample documents from the test set\n",
    "for idx in (3,8,13,18,23,28,33,38):\n",
    "    identifier = test_ids[idx]\n",
    "    text = test_x[idx]\n",
    "    metadata = test_y[idx]\n",
    "    print(identifier)\n",
    "    print(\"---\")\n",
    "    print(\"DSpace metadata:\")\n",
    "    print(metadata_to_text(metadata))\n",
    "    print(\"---\")\n",
    "    print(\"Generated metadata:\")\n",
    "    gen_metadata = get_completions(text).strip()\n",
    "    print(gen_metadata)\n",
    "    print()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dd50b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
